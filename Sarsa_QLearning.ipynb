{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FSvAv4WngOO"
      },
      "source": [
        "# Sarsa & Q Learning\n",
        "In this exercise you are going to implement two algorithms: `SARSA` and `Q-Learning`.\n",
        "Both `SARSA` and `Q-Learning` are examples of `Temporal Diference` (TD) algorithms. The main diference is in update rule. `SARSA` is an On-policy method - we are computing $TD_{target}$ using the same policy that we used for explorarion ($\\epsilon-Greedy$):\n",
        "\n",
        "$Q(S_t, A_t) = Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$\n",
        "\n",
        "whereas in `Q-Learning` we are using $\\epsilon-Greedy$ policy to explore, and $Greedy$ policy to compute $TD_{target}$:\n",
        "\n",
        "$Q(S_t, A_t) = Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma \\max\\limits_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)$\n",
        "\n",
        "In the first two task you are going to work with the [Cliff Walking](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/) environment, whereas in task 3 the [Taxi](https://www.gymlibrary.dev/environments/toy_text/taxi/) enviornment will be used.\n",
        "\n",
        "\n",
        "### Tasks Overview\n",
        "1. Task 1: SARSA and [Cliff Walking](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/) environment.\n",
        "\n",
        "2. Task 2: Q-Learning and [Cliff Walking](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/) environment\n",
        "    - In `agents.py` complete the `QLearningAgent` class (you must compliete `get_action` and `update` methods)\n",
        "    - set the parameters $\\alpha$, $\\gamma$, and $\\epsilon$ for the `qAgent`\n",
        "    - Complete the learning loop in this notebook. Run it and train the agent.\n",
        "    - Test the agent. You may test it by using `python render.py -a QLearning` from the command line. Make sure that your agent chooses actions correctly.\n",
        "3. Task 3. [Taxi](https://www.gymlibrary.dev/environments/toy_text/taxi/) enviornment\n",
        "    - Apply both `SARSA` and `QLearning` agents for the [Taxi](https://www.gymlibrary.dev/environments/toy_text/taxi/) enviornment. Test the results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfiy2LeZngOS",
        "outputId": "abe74757-d4fe-479c-9bb7-0b1a3f1eaa3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-004019101115>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSarsaAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQLearningAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'agents'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from agents import SarsaAgent, QLearningAgent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmbvcd15ngOT"
      },
      "source": [
        "## Cliff walking Environment.\n",
        "![alt text](https://www.gymlibrary.dev/_images/cliff_walking.gif)\n",
        "\n",
        "### Task 1 - SARSA\n",
        "1. In `agents.py` complete the `SarsaAgent` class (you must complete `get_action` and `update` methods).\n",
        "2. Set the parameters $\\alpha$, $\\gamma$, and $\\epsilon$ for the `sarsaAgent`.\n",
        "3. Complete the learning loop in this notebook. Run it and train the agent.\n",
        "5. Test the agent. You may test it by using `python render.py -a SARSA` from the command line. Make sure that your agent chooses actions correctly. Change the parameters from point 2 or increase the number of training episodes if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FehFAU5OngOU"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CliffWalking-v0')\n",
        "# YOUR CODE HERE\n",
        "# Set the parameters of the agent\n",
        "sarsaAgent = SarsaAgent(env, alpha=0, gamma=0, epsilon=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0GFu8xFngOU"
      },
      "outputs": [],
      "source": [
        "sarsa_Rewards = []\n",
        "# YOUR CODE HERE\n",
        "# Change the number of episodes, if necessary\n",
        "for episode in tqdm(range(100)):\n",
        "    episode_reward = 0\n",
        "    state, info = env.reset()\n",
        "\n",
        "    # Use the agent to choose the first action\n",
        "    action = None # YOUR CODE HERE\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Perform the selected action\n",
        "        next_state, reward, done, truncated, info = None # YOUR CODE HERE\n",
        "        # Select next action\n",
        "        next_action = None # YOUR CODE HERE\n",
        "        # Update the agent\n",
        "\n",
        "\n",
        "\n",
        "        # Assign the next state and action to the current ones\n",
        "        state = None # YOUR CODE HERE\n",
        "        action = None # YOUR CODE HERE\n",
        "        episode_reward += reward\n",
        "    sarsa_Rewards.append(episode_reward)\n",
        "\n",
        "sarsaAgent.save()\n",
        "plt.plot(sarsa_Rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYR8LifzngOU"
      },
      "source": [
        "### Task 2 - Q-Learning\n",
        "1. In `agents.py` complete the `QLearningAgent` class (you must complete `get_action` and `update` methods)\n",
        "2. Set the parameters $\\alpha$, $\\gamma$, and $\\epsilon$ for the `qAgent`\n",
        "3. Complete the learning loop in this notebook. Run it and train the agent.\n",
        "4. Test the agent. You may test it by using `python render.py -a QLearning` from the command line. Make sure that your agent chooses actions correctly. Change the parameters from point 2 or increase the number of training episodes if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEYhEmdVngOV"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Set the parameters of the agent\n",
        "qAgent = QLearningAgent(env, alpha=0.1, gamma=0.95, epsilon=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNn1hAOongOV"
      },
      "outputs": [],
      "source": [
        "Q_rewards = []\n",
        "for episode in tqdm(range(100)):\n",
        "    episode_reward = 0\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Choose an action\n",
        "        action = None # YOUR CODE HERE\n",
        "\n",
        "        # Perform the selected action\n",
        "        next_state, reward, done, truncated, info = None # YOUR CODE HERE\n",
        "\n",
        "        # Update the agent\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Assign the next state to the current one\n",
        "        state = None # YOUR CODE HERE\n",
        "\n",
        "        episode_reward += reward\n",
        "    Q_rewards.append(episode_reward)\n",
        "\n",
        "qAgent.save()\n",
        "plt.plot(Q_rewards)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVe7ttJGngOV"
      },
      "source": [
        "## Taxi Environment\n",
        "![alt text](https://www.gymlibrary.dev/_images/taxi.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MpKGmjZngOV"
      },
      "source": [
        "### - Task 3 - Taxi environment\n",
        "- Apply both `SARSA` and `QLearning` agents for the [Taxi](https://www.gymlibrary.dev/environments/toy_text/taxi/) enviornment. Note that you may want to save the $Q_{estimated}$ values in the different files. You can do it using a parameter in `agent.save` method, for example: `sarsaAgent.save('Taxi_Sarsa.npy')` and `QAgent.save('Taxi_Q.npy')`\n",
        "- Test the agents. If you saved the parameters as above, you may test them using `python render.py -a SARSA -e Taxi-v3 -f Taxi_Sarsa.npy` for SARSA and `python render.py -a QLearning -e Taxi-v3 -f Taxi_Q.npy` for Q-Learning. Make sure that your agent chooses actions correctly. Change the agent parameters the number of training episodes if necessary.\n",
        "\n",
        "\n",
        "*Note: I noticed that the environment sometimes does not render correctly (rendering is not displayed despite the correct actions taken). If you observe something like this double-check the printed states - it may be simply the environment error.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcdQe7gZngOV"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lf1D5OsngOW"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8YDWR21ngOW"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gym",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}